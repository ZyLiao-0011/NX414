{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38899ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import load_it_data, visualize_img\n",
    "import h5py\n",
    "import os\n",
    "device = torch.device(\"mpc\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3421a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '' ## Insert the folder where the data is, if you download in the same folder as this notebook then leave it blank\n",
    "\n",
    "stimulus_train, stimulus_val, stimulus_test, objects_train, objects_val, objects_test, spikes_train, spikes_val = load_it_data(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ca3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "objects_train_encoded = label_encoder.fit_transform(objects_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562024cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeData(Dataset):\n",
    "    def __init__(self,stimulus,objects,label_encoder = None):\n",
    "        self.stimulus = torch.tensor(stimulus)\n",
    "        #self.objects = objects\n",
    "        #self.labels = label_encoder.transform(objects)\n",
    "        self.labels = torch.tensor(label_encoder.transform(objects))\n",
    "        #self.spikes = spikes if spikes is not None else np.zeros((stimulus.shape[0], stimulus.shape[1], 1))\n",
    "        \n",
    "        self.number_of_class = np.unique(objects).shape[0] \n",
    "    def __len__(self):\n",
    "        return len(self.stimulus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.stimulus[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7befa986",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT5dataloader_train = DataLoader(SpikeData(stimulus_train, objects_train, label_encoder=label_encoder), batch_size=64, shuffle=True)\n",
    "IT5dataloader_val = DataLoader(SpikeData(stimulus_val,  objects_val, label_encoder=label_encoder), batch_size=64, shuffle=False)\n",
    "IT5dataloader_test = DataLoader(SpikeData(stimulus_test, objects_test,label_encoder=label_encoder), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afe6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ShallowCNN, self).__init__()\n",
    "\n",
    "        # Conv block 1\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)   # (3, 224, 224) → (32, 224, 224)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv block 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # (64, 112, 112)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Conv block 3\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) # (128, 56, 56)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Compute flattened size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 224, 224)\n",
    "            x = self._forward_features(dummy_input)\n",
    "            self.flattened_size = x.view(1, -1).shape[1]\n",
    "\n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)   # (32, 112, 112)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)   # (64, 56, 56)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)   # (128, 28, 28)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod # defines that the following function does not take self as input\n",
    "    @torch.no_grad() # ensures that following function runs without tracking gradients\n",
    "    # making the initialisation faster and more memory-efficient. The parameters remain trainable.\n",
    "    def init_weights(module):\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "        # He initialization\n",
    "        # Conv2d layers\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        # Linear layers\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "        # Initialize BatchNorm weights to 1 and bias to 0\n",
    "        elif isinstance(module, nn.BatchNorm2d):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d64f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=2):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "            for stimulus, labels in train_loader:\n",
    "                stimulus, labels = stimulus.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(stimulus)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                pbar.set_postfix(loss=running_loss/len(train_loader))\n",
    "                pbar.update(1)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for stimulus, labels in val_loader:\n",
    "                stimulus, labels = stimulus.to(device), labels.to(device)\n",
    "                outputs = model(stimulus)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c0c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 41/41 [01:39<00:00,  2.44s/it, loss=104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 104.1130\n",
      "Validation Loss: 4.1589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 41/41 [01:36<00:00,  2.35s/it, loss=4.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Loss: 4.1605\n",
      "Validation Loss: 4.1592\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = ShallowCNN(num_classes=64)\n",
    "model.apply(ShallowCNN.init_weights)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
    "train_model(model, IT5dataloader_train, IT5dataloader_val, loss_fn, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9060b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "layer_outputs = defaultdict(list)\n",
    "all_labels = []\n",
    "\n",
    "def save_output(name):\n",
    "    def hook(module, input, output):\n",
    "        # Save the detached output for this batch\n",
    "        layer_outputs[name].append(output.detach().cpu())\n",
    "    return hook\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Register forward hooks for the layers you want\n",
    "model.conv1.register_forward_hook(save_output('conv1'))\n",
    "model.conv2.register_forward_hook(save_output('conv2'))\n",
    "model.conv3.register_forward_hook(save_output('conv3'))\n",
    "model.fc1.register_forward_hook(save_output('fc1')) \n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in IT5dataloader_val:  # all batches\n",
    "        _ = model(images)  # triggers the hooks\n",
    "        all_labels.append(labels.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53da7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_matrices = {}\n",
    "for layer_name, outputs in layer_outputs.items():\n",
    "    # outputs is a list of [batch_size, C, H, W] or [batch_size, D]\n",
    "    flat = [out.view(out.size(0), -1) for out in outputs]  # flatten per batch\n",
    "    layer_matrix = torch.cat(flat, dim=0)  # (n_samples, n_features)\n",
    "    layer_matrices[layer_name] = layer_matrix\n",
    "\n",
    "# Also flatten all labels\n",
    "all_labels = torch.cat(all_labels, dim=0)  # shape: (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a1aa2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0321, -0.1428, -0.1428,  ..., -0.0802, -0.0802, -0.0733],\n",
       "        [-0.0321, -0.1428, -0.1428,  ..., -0.0802, -0.0802, -0.0733],\n",
       "        [-0.0321, -0.1428, -0.1428,  ..., -0.0802, -0.0802, -0.0733],\n",
       "        ...,\n",
       "        [-0.0321, -0.1428, -0.1428,  ..., -0.0802, -0.0802, -0.0733],\n",
       "        [-0.0321, -0.1428, -0.1428,  ..., -0.0802, -0.0802, -0.0733],\n",
       "        [-0.0321, -0.1428, -0.1428,  ..., -0.0802, -0.0802, -0.0733]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_matrices['conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903767f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.model_selection import cross_validate,KFold\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=1000)\n",
    "\n",
    "X_train = scaler.fit_transform(layer_matrices['conv1'])\n",
    "X_train.shape\n",
    "#X_train = pca.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00f1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "#from scipy.stats import pearsonr\n",
    "# Define a scoring function compatible with make_scorer\n",
    "def pearson_score(y_true, y_pred):\n",
    "    scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        r = np.corrcoef(y_true[:, i], y_pred[:, i])[0, 1]\n",
    "        if np.isnan(r):\n",
    "            r = 0\n",
    "        if np.isinf(r):\n",
    "            r = 0\n",
    "        scores.append(r)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Wrap it as a scorer (greater_is_better=True is default)\n",
    "pearson_scorer = make_scorer(pearson_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43085825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "\n",
    "# Ridge model\n",
    "ridge = Ridge(max_iter=10000, tol=1e-6)\n",
    "\n",
    "# Grid of alpha values\n",
    "param_grid = {'alpha': np.logspace(-4, 4, 10)}\n",
    "\n",
    "# GroupKFold ensures the same label distribution in each fold\n",
    "cv = GroupKFold(n_splits=10)\n",
    "\n",
    "# Wrap with GridSearchCV, pass groups\n",
    "grid = GridSearchCV(ridge, param_grid, cv=cv, scoring=pearson_scorer, n_jobs=-1)\n",
    "\n",
    "# Fit with groups controlling label distribution\n",
    "grid.fit(X_train_pca, spikes_train, groups=objects_train_encoded)\n",
    "\n",
    "# Results\n",
    "print(\"Best alpha:\", grid.best_params_['alpha'])\n",
    "print(\"Best pearson correlation:\", grid.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
